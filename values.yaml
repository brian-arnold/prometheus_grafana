prometheus:
  prometheusSpec:
    nodeSelector:
      kubernetes.io/hostname: at-compute010
    retention: 21d
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: prometheus-storage 
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    
    additionalScrapeConfigsSecret:
      enabled: true
      # for MinIO, put scrape config in a secret b/c it requires a token
      name: additional-scrape-configs-dpv-minio
      key: prometheus-dpv-minio.yaml

    # this worked
    # additionalScrapeConfigs:
    #   - job_name: 'directpv-metrics'
    #     scheme: http  # Confirmed HTTP works
    #     metrics_path: /directpv/metrics
    #     kubernetes_sd_configs:
    #     - role: pod
    #       namespaces:
    #         names:
    #         - directpv
    #     relabel_configs:
    #     # Keep only node-server pods (they have the metrics port)
    #     - source_labels: [__meta_kubernetes_pod_name]
    #       regex: "node-server-.*"
    #       action: keep
    #     # Set the correct port (10443)
    #     - source_labels: [__address__]
    #       regex: '([^:]+)(?::\d+)?'
    #       target_label: __address__
    #       replacement: '${1}:10443'
    #     # Add node name as label
    #     - source_labels: [__meta_kubernetes_pod_node_name]
    #       target_label: node

  # add additional serive monitor to allow prometheus to scrape metrics from nvidia-dcgm-exporter
  additionalServiceMonitors:
    - name: nvidia-dcgm-exporter
      selector:
        matchLabels:
          app: nvidia-dcgm-exporter  # Adjust this to match your service's labels
      namespaceSelector:
        matchNames:
          - gpu-operator
      endpoints:
        - port: gpu-metrics
          interval: 15s

prometheus-node-exporter:
  # allow more unavaiable nodes to account for some problematic ones that may not restart
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 5
  # this section here was taken from the default values file and modified to exclude |mnt/lbm[0-9]+ 
  extraArgs:
    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+|mnt/lbm[0-9]+)($|/)
    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$

grafana:
  sidecar:
    dashboards:
      annotations:
        grafana_folder: "Defaults"  # Dashboards with this annotation will be put in this folder
      folderAnnotation: "grafana_folder" # custom dashboards can be organized by using the grafana_folder label in your configmap.
      provider:
       foldersFromFilesStructure: true
    alerts:
      enabled: true
      label: grafana_alert
      labelValue: "1"
      searchNamespace: ALL
  # Disable init container that sets ownership - conflicts with existing hostPath data
  initChownData:
    enabled: false
  nodeSelector:
    kubernetes.io/hostname: at-compute010
  persistence:
    enabled: true
    storageClassName: grafana-storage
    size: 2Gi
  # added to allow embedding Grafana panels in Ray Dashboard, 
  # have grafana.ini->auth.anonymous->enabled: true and grafana.ini->auth.anonynmous->org_role: Viewer
  # however, this also allows anyone access to the URL visibility into your dashboards
  grafana.ini:
    security:
      allow_embedding: true
      admin_user: admin  
    auth.anonymous:
      enabled: false
      org_role: Viewer