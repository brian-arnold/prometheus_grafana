apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: gpu-alerts
  namespace: monitoring
  labels:
    release: prometheus  # This label is key for discovery
spec:
  groups:
    - name: gpu.rules
      rules:
        - alert: GPUHighCoreTemp
          expr: DCGM_FI_DEV_GPU_TEMP >= 92
          for: 1m
          labels:
            severity: critical
            team: enigma
          annotations:
            summary: "Node {{ $labels.Hostname }} has high GPU core temp"
            description: "GPU {{ $labels.gpu }} on {{ $labels.Hostname }} has temp {{ $value }}C"

        - alert: GPUHighJunctionTemp
          expr: DCGM_FI_DEV_MEMORY_TEMP >= 100
          for: 1m
          labels:
            severity: critical
            team: enigma
          annotations:
            summary: "Node {{ $labels.Hostname }} has high GPU junction temp"
            description: "GPU {{ $labels.gpu }} on {{ $labels.Hostname }} has temp {{ $value }}C"

        - alert: GPUUnhealthy
          expr: DCGM_FI_DEV_XID_ERRORS > 0
          for: 5m
          labels:
            severity: warning
            team: enigma
          annotations:
            summary: "Node {{ $labels.Hostname }} has GPU XID errors"
            description: |
              GPU {{ $labels.gpu }} on `{{ $labels.Hostname }}` has XID errors: {{ $value }}. 
              Container `{{ $labels.exported_container}}` in namespace `{{ $labels.exported_namespace }}` may be affected.
              See `https://docs.nvidia.com/deploy/xid-errors/analyzing-xid-catalog.html` for details.
              If container and namespace names are empty, nothing is using the GPU currently.

        - alert: GPUCountMismatch
          expr: |
            label_replace(kube_node_status_capacity{resource="nvidia_com_gpu"}, "Hostname", "$1", "node", "(.*)")
            != on(Hostname)
            count by (Hostname) (DCGM_FI_DEV_GPU_UTIL)
          for: 5m
          labels:
            severity: warning
            team: enigma
          annotations:
            summary: "GPU count mismatch on {{ $labels.Hostname }}"
            description: |
              K8s reports {{ $value }} GPUs but DCGM reports a different count.  
              To confirm: compare `kubectl describe node {{ $labels.Hostname }}` with `nvidia-smi` output.
              To fix: restart nvidia-device-plugin-daemonset pod on `{{ $labels.Hostname }}`.

        - alert: DCGMExporterDown
          expr: up{job="nvidia-dcgm-exporter"} == 0
          for: 5m
          labels:
            severity: warning
            team: enigma
          annotations:
            summary: "DCGM exporter {{ $labels.pod }} is down"
            description: |
              nvidia-dcgm-exporter {{ $labels.pod }} is not responding.
              See pod status and affected node: `kubectl get pod {{ $labels.pod }} -n gpu-operator -o wide`